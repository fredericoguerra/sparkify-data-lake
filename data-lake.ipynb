{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake for Data Song Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Data lake set up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-to'], dest='OUTPUT_BUCKET', nargs=None, const=None, default=None, type=None, choices=None, help='Path of the bucket to write final tables.', metavar=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--key', action='store', dest = 'AWS_ACCESS_KEY_ID',\n",
    "                    required = True, help = 'AWS Access Key ID of the IAM user')\n",
    "parser.add_argument('--secret', action='store', dest = 'AWS_SECRET_ACCESS_KEY',\n",
    "                    required = True, help = 'AWS Secret Access Key of the IAM user')\n",
    "parser.add_argument('-from', action='store', dest = 'INPUT_BUCKET',\n",
    "                    required = False, default = 's3a://udacity-dend/', help = 'Path of the bucket to read files from.')\n",
    "parser.add_argument('-to', action='store', dest = 'OUTPUT_BUCKET',\n",
    "                    required = True, help = 'Path of the bucket to write final tables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --key AWS_ACCESS_KEY_ID --secret\n",
      "                             AWS_SECRET_ACCESS_KEY [-from INPUT_BUCKET] -to\n",
      "                             OUTPUT_BUCKET\n",
      "ipykernel_launcher.py: error: the following arguments are required: --key, --secret, -to\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "arguments = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "input_data = config['AWS']['INPUT_DATA']\n",
    "output_data = config['AWS']['OUTPUT_DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process song data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "song_data = 'song_data/A/B/C/TRABCEI128F424C983.json'\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(f\"{input_data}{song_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "songs_table = df.select(['song_id','title','artist_id','year','duration']).dropDuplicates(['song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode('overwrite').partitionBy('year','artist_id').parquet(output_data + 'songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "artists_table = df.select(['artist_id','artist_name','artist_location',\\\n",
    "                           'artist_latitude','artist_longitude']).dropDuplicates(['artist_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.mode(\"overwrite\").parquet(output_data + 'artists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = \"log_data/2018/11/2018-11-13-events.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df = spark.read.json(f\"{input_data}{log_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.where('page = \"NextSong\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('log_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table  \n",
    "users_table = df.select(['userId','firstName','lastName','gender','level']).dropDuplicates(['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.mode('overwrite').parquet(output_data + 'users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda ts: str(int(ts/1000.0)))\n",
    "df = df.withColumn('timestamp',get_timestamp(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda ts: str(datetime.fromtimestamp(int(ts)/1000.0)))\n",
    "df = df.withColumn(\"datetime\", get_datetime(df.ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda ts: str(datetime.fromtimestamp(int(ts)/1000.0)))\n",
    "df = df.withColumn(\"datetime\", get_datetime(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = df.select(\n",
    "col('datetime').alias('start_time'),\n",
    "hour('datetime').alias('hour'),\n",
    "dayofmonth('datetime').alias('day'),\n",
    "weekofyear('datetime').alias('week'),\n",
    "month('datetime').alias('month'),\n",
    "year('datetime').alias('year')\n",
    ").dropDuplicates(['start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode('overwrite').partitionBy('year','month').parquet(output_data + 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # read in song data to use for songplays table\n",
    "song_data = output_data + \"songs\"\n",
    "song_df = spark.read.parquet(song_data)\n",
    "songplays_df = df.join(song_df, (song_df.title == df.song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = songplays_df.select(\n",
    "col('ts').alias('start_time'),\n",
    "col('userId').alias('user_id'),\n",
    "col('level').alias('level'),\n",
    "col('song_id').alias('song_id'),\n",
    "col('artist_id').alias('artist_id'),\n",
    "col('sessionId').alias('session_id'),\n",
    "col('artist_location').alias('location'),\n",
    "col('userAgent').alias('user_agent'),\n",
    "year('datetime').alias('year'),\n",
    "month('datetime').alias('month')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = songplays_table.withColumn('songplay_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.mode('overwrite').partitionBy('year','month').parquet(output_data + 'songplays')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
